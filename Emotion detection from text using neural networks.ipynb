{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20bf74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you have these versions of TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5123fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b477834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e0b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "train=pd.read_table('train.txt', delimiter = ';', header=None, )\n",
    "val=pd.read_table('val.txt', delimiter = ';', header=None, )\n",
    "test=pd.read_table('test.txt', delimiter = ';', header=None, )\n",
    "\n",
    "data = pd.concat([train ,  val , test])\n",
    "data.columns = [\"text\", \"label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c44756fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3042a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eac3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(line):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', line) #leave only characters from a to z\n",
    "    review = review.lower() #lower the text\n",
    "    review = review.split() #turn string into list of words\n",
    "    #apply Stemming \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #delete stop words like I, and ,OR   review = ' '.join(review)\n",
    "    #trun list into sentences\n",
    "    return \" \".join(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d230e70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f32480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['N_label'] = label_encoder.fit_transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c8fea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       didnt feel humili\n",
       "1       go feel hopeless damn hope around someon care ...\n",
       "2                    im grab minut post feel greedi wrong\n",
       "3          ever feel nostalg fireplac know still properti\n",
       "4                                            feel grouchi\n",
       "                              ...                        \n",
       "1995    keep feel like someon unkind wrong think get b...\n",
       "1996              im feel littl cranki neg doctor appoint\n",
       "1997                feel use peopl give great feel achiev\n",
       "1998    im feel comfort derbi feel though start step s...\n",
       "1999    feel weird meet w peopl text like dont talk fa...\n",
       "Name: text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "596e2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model by applying Countvectorizer -convert textual data to numerical data\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))#example: the course was long-> [the,the course,the course was,course, course was, course was long,...]\n",
    "\n",
    "data_cv = cv.fit_transform(data['text']).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "273c3e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29d54b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test=data_cv,test_cv,train['N_label'],test['N_label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(data_cv, data['N_label'], test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33985ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HP\\anaconda3\\envs\\testeni\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 4s 236us/sample - loss: 0.9544 - acc: 0.6527\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 3s 195us/sample - loss: 0.3031 - acc: 0.9037\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 3s 181us/sample - loss: 0.1665 - acc: 0.9465\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 3s 183us/sample - loss: 0.1064 - acc: 0.9670\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 3s 190us/sample - loss: 0.0724 - acc: 0.9783\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 3s 212us/sample - loss: 0.0527 - acc: 0.9833\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 3s 213us/sample - loss: 0.0395 - acc: 0.9883\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 3s 191us/sample - loss: 0.0295 - acc: 0.9904\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 3s 197us/sample - loss: 0.0239 - acc: 0.9931\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 3s 196us/sample - loss: 0.0222 - acc: 0.9931\n",
      "15000/15000 [==============================] - 1s 66us/sample - loss: 0.0145 - acc: 0.9961\n",
      "Accuracy: 99.61\n"
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# load the dataset\n",
    "# split into input (X) and output (y) variables\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6867f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 67us/sample - loss: 0.8574 - acc: 0.8518\n",
      "Accuracy: 85.18\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79c5ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fee946e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='I feel sad'\n",
    "text=preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = model.predict(array)\n",
    "a=np.argmax(pred, axis=1)\n",
    "label_encoder.inverse_transform(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5877746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,'my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea79609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(label_encoder, open('encoder.pkl', 'wb'))\n",
    "pickle.dump(cv, open('CountVectorizer.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add5250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testeni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
